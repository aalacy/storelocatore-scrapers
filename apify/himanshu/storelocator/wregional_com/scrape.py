import csv
from bs4 import BeautifulSoup as bs
from sgrequests import SgRequests
from sglogging import sglog
import re
from itertools import groupby
import json

DOMAIN = "wregional.com"
BASE_URL = "https://www.wregional.com"
LOCATION_URL = [
    "https://www.wregional.com/main/find-a-facility-or-clinic",
    "https://www.wregional.com/main/medical-services--programs",
]
HEADERS = {
    "Accept": "application/json, text/plain, */*",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36",
}
log = sglog.SgLogSetup().get_logger(logger_name=DOMAIN)

session = SgRequests()


def write_output(data):
    log.info("Write Output of " + DOMAIN)
    with open("data.csv", mode="w") as output_file:
        writer = csv.writer(
            output_file, delimiter=",", quotechar='"', quoting=csv.QUOTE_ALL
        )
        # Header
        writer.writerow(
            [
                "locator_domain",
                "page_url",
                "location_name",
                "street_address",
                "city",
                "state",
                "zip",
                "country_code",
                "store_number",
                "phone",
                "location_type",
                "latitude",
                "longitude",
                "hours_of_operation",
            ]
        )
        # Body
        for row in data:
            writer.writerow(row)


def pull_content(url):
    log.info("Pull content => " + url)
    soup = bs(session.get(url, headers=HEADERS).content, "lxml")
    return soup


def handle_missing(field):
    if field is None or (isinstance(field, str) and len(field.strip()) == 0):
        return "<MISSING>"
    return field


def parse_json(soup):
    info = soup.find("script", type="application/ld+json").string
    data = json.loads(info)
    return data


def is_multiple(street_address, location_name, locations):
    for row in locations:
        if street_address in row and location_name in row:
            log.info("Found multiple address => " + street_address)
            return True
    return False


def fetch_store_urls():
    log.info("Fetching store URL")
    store_urls = [
        {
            "url": "https://www.wregional.com/main/dialysis-centers-locations",
            "type": "",
            "name": "",
        }
    ]
    for home_url in LOCATION_URL:
        soup = pull_content(home_url)
        uls = soup.find_all("ul", {"class": "clinics"})
        for ul in uls:
            href = ul.find_all("a")
            for row in href:
                url = (
                    BASE_URL + row["href"]
                    if "urgentteam.com" not in row["href"]
                    and "wregional.com" not in row["href"]
                    else row["href"]
                )
                if "find-a-facility-or-clinic" in home_url:
                    type = row.find_parent("div").parent.find("h3").text
                    info = {
                        "url": url,
                        "type": type,
                        "name": row.parent.text,
                    }
                else:
                    type = row.parent.text
                    info = {
                        "url": url,
                        "type": type,
                        "name": type,
                    }
                store_urls.append(info)
    log.info("Found {} URL ".format(len(store_urls)))
    return store_urls


def get_hoo(link):
    soup = pull_content(link)
    hoo = soup.find("div", {"class": "branch-hours"}).find_all("p")
    hoo = hoo[1].find("span")
    hours_of_operations = "{}: {}".format(hoo.text, hoo.next_sibling.strip())
    return hours_of_operations


def fetch_data():
    except_link = ["https://www.wregional.com/hospice/willard-walker-hospice-home.aspx"]
    log.info("Fetching store_locator data")
    store_urls = fetch_store_urls()
    locations = []
    for row in store_urls:
        page_url = row["url"]
        if page_url in except_link:
            continue
        soup = pull_content(page_url)
        if "dialysis-centers-locations" in page_url:
            contents = soup.find("div", {"class": "page-content"}).find_all("strong")
            for content in contents:
                location_name = content.text
                street_address = content.next_sibling.next_sibling.strip()
                info = content.next_sibling.next_sibling.next_sibling.next_sibling.strip().split(
                    ","
                )
                city = info[0]
                state = info[1].split(" ")[1]
                zip_code = info[1].split(" ")[2]
                country_code = "US"
                store_number = "<MISSING>"
                phone = content.find_next("a").text
                hours_of_operation = (
                    content.find_next("a")
                    .next_sibling.find_next(text=True)
                    .find_next(text=True)
                    .strip()
                    .replace(" or later as needed for patient care", "")
                    .replace("â€“", "-")
                )
                if "Dialysis Center" in location_name:
                    location_type = "DIALYSIS_CENTER"
                elif "Home Dialysis" in location_name:
                    location_type = "HOME_DIALYSIS"
                else:
                    location_type = "<MISSING>"
                latitude = "<MISSING>"
                longitude = "<MISSING>"
                if not is_multiple(street_address, location_name, locations):
                    locations.append(
                        [
                            DOMAIN,
                            "https://www.wregional.com/main/dialysis-centers-locations",
                            location_name,
                            street_address,
                            city,
                            state,
                            zip_code,
                            country_code,
                            store_number,
                            phone,
                            location_type,
                            latitude,
                            longitude,
                            hours_of_operation,
                        ]
                    )
        elif "springdale-center-for-health" in page_url or "medical-plaza" in page_url:
            content = soup.find("div", {"class": "page-content"})
            location = fetch_type_b(content, page_url, row["type"], row["name"])
            if not is_multiple(location[3], location[2], locations):
                locations.append(location)
        elif "urgentteam.com" in page_url:
            location = fetch_type_urgent(soup, page_url)
            if not is_multiple(location[3], location[2], locations):
                locations.append(location)
        elif soup.find("div", {"class": "clinics"}) and soup.find(
            "div", {"class": "clinics"}
        ).find("a", text=re.compile(r"Map and Directions|Map and Driving Directions")):
            content = soup.find("div", {"class": "page-content"})
            location = fetch_type_a(content, page_url, row["type"], row["name"])
            for val in location:
                if isinstance(val, list):
                    if not is_multiple(val[3], val[2], locations):
                        locations.append(val)
                else:
                    if not is_multiple(location[3], location[2], locations):
                        locations.append(location)
                    break
    return locations


def fetch_type_a(content, page_url, location_type, location_name):
    full_address = (
        content.find("div", class_="clinics")
        .find("p")
        .get_text(strip=True, separator="|")
    )
    if len(full_address) <= 1:
        full_address = content.find("div", class_="col-2").get_text(
            strip=True, separator="|"
        )
    full_address = full_address.replace("Telephone (Washington County):", "Telephone:")
    full_address = full_address.replace("Telephone: ", "Telephone:|")
    full_address = full_address.replace("Phone:", "Telephone:").split("|")
    if len(full_address) <= 1:
        return fetch_type_a2(content, page_url, location_type, location_name)
    location = fetch_type_a_proccess(
        page_url, content, full_address, location_name, location_type
    )
    return location


def fetch_type_a2(content, page_url, location_type, location_name):
    locations = []
    main = content.find("div", {"class": "clinics"}).find("div", {"class": "col-1"})
    if not main:
        return fetch_type_a3(content, page_url, location_type, location_name)
    else:
        main = main.find_all("h2")
    for row in main:
        url = row.find("a")
        page_url = (
            BASE_URL + url["href"]
            if "urgentteam.com" not in url["href"]
            and "wregional.com" not in url["href"]
            else url["href"]
        )
        soup = pull_content(page_url)
        if not soup.find("div", class_="clinics"):
            continue
        full_address = (
            soup.find("div", class_="clinics")
            .find("p")
            .get_text(strip=True, separator="|")
        )
        if len(full_address) <= 1:
            el = soup.find("div", class_="col-2")
            full_address = el.get_text(strip=True, separator="|")
            if len(el.find_all("h2")) > 1:
                full_address1 = full_address.replace(
                    "Telephone (Washington County):", "Telephone:"
                )
                full_address1 = full_address1.replace("Telephone: ", "Telephone:|")
                full_address1 = full_address1.replace("Phone:", "Telephone:").split("|")
                i = (
                    list(g)
                    for _, g in groupby(full_address1, key="Map and Directions".__ne__)
                )
                full_address_list = [a + b for a, b in zip(i, i)]
                for row_full_address in full_address_list:
                    row_full_address = "|".join(row_full_address)
                    row_full_address = row_full_address.replace(
                        "Telephone (Washington County):", "Telephone:"
                    )
                    row_full_address = row_full_address.replace(
                        "Telephone: ", "Telephone:|"
                    )
                    row_full_address = row_full_address.replace(
                        "Phone:", "Telephone:"
                    ).split("|")
                    location = fetch_type_a_proccess(
                        page_url,
                        soup,
                        row_full_address,
                        location_name,
                        location_type,
                    )
                    locations.append(location)
            else:
                full_address = full_address.replace(
                    "Telephone (Washington County):", "Telephone:"
                )
                full_address = full_address.replace("Telephone: ", "Telephone:|")
                full_address = full_address.replace("Phone:", "Telephone:").split("|")
                location = fetch_type_a_proccess(
                    page_url,
                    soup,
                    full_address,
                    location_name,
                    location_type,
                )
                locations.append(location)
    return locations


def fetch_type_a3(content, page_url, location_type, location_name):
    main = content.find("div", {"class": "clinics"}).find(
        "ul", {"class": "providers flex"}
    )
    if not main:
        info_addr = (
            content.find("div", {"class": "clinics"})
            .get_text(strip=True, separator="|")
            .split("|")
        )
        if len(info_addr) <= 17:
            del info_addr[:10]
        else:
            del info_addr[:17]
        info_addr = "|".join(info_addr)
        info_addr = info_addr.encode("ascii", "ignore").decode()
    else:
        info_addr = main.find_next("div", {"class": "grey-line"}).find_next("p")
        if not info_addr:
            info_addr = (
                content.find("div", {"class": "clinics"})
                .get_text(strip=True, separator="|")
                .split("|")
            )
            del info_addr[:86]
            info_addr = "|".join(info_addr)
        else:
            info_addr = info_addr.get_text(strip=True, separator="|")
    full_address = info_addr.replace("Telephone (Washington County):", "Telephone:")
    full_address = full_address.replace("Telephone: ", "Telephone:|")
    full_address = full_address.replace("Phone:", "Telephone:").split("|")
    location = fetch_type_a_proccess(
        page_url,
        content,
        full_address,
        location_name,
        location_type,
    )
    return location


def fetch_type_a_proccess(
    page_url, content, full_address, location_name, location_type
):
    formated_address = full_address[: full_address.index("Telephone:") + 2]
    if (
        "(" in formated_address[1]
        and "3 E. Appleby Road" not in formated_address[1]
        or "Springdale Center for Health" in formated_address[1]
        or "Located in the William L. Bradley Medical Plaza" in formated_address[1]
    ):
        del formated_address[1]
    if len(formated_address) >= 5:
        location_name = formated_address[0]
        street_address = formated_address[1]
        city = formated_address[2].split(",")[0]
        state = formated_address[2].split(",")[1].strip().split()[0]
        zip_code = formated_address[2].split(",")[1].strip().split()[1]
        phone_tag = formated_address[4].replace("Telephone:", "")
    else:
        street_address = formated_address[0]
        city = formated_address[1].split(",")[0]
        state = formated_address[1].split(",")[1].strip().split()[0]
        zip_code = formated_address[1].split(",")[1].strip().split()[1]
        phone_tag = formated_address[3].replace("Telephone:", "")
    phone_list = re.findall(
        re.compile(
            r"(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4}|\(\d{3}\)\s*\d{3}[-\.\s]??\d{4}|\d{3}[-\.\s]??\d{4})"
        ),
        str(phone_tag),
    )
    if phone_list:
        phone = phone_list[0]
    else:
        phone = "<MISSING>"
    store_number = "<MISSING>"
    country_code = "US"
    latitude = "<MISSING>"
    longitude = "<MISSING>"
    regex = re.compile(r"Monday.*|Thursday.*")
    find_hoo = list(filter(regex.match, full_address))
    if find_hoo:
        hours_of_operation = " - ".join(find_hoo) if len(find_hoo) > 1 else find_hoo[0]
    elif content.find(
        lambda tag: (tag.name == "h3" or tag.name == "h2")
        and "Hours" == tag.text.strip()
    ):
        hours_of_operation = (
            " ".join(
                list(
                    content.find(
                        lambda tag: (tag.name == "h3" or tag.name == "h2")
                        and "Hours" == tag.text.strip()
                    ).nextSibling.next_sibling.stripped_strings
                )
            )
            .split("Other clinic")[0]
            .replace("Hours at", "")
            .replace("\n", " ")
            .replace("Fayetteville:", "")
            .replace("Springdale", "")
            .split("The clinic is")[-1]
            .replace("The Fayetteville clinic is", "")
            .replace(
                "other clinic locations vary. For more information, please call 479-571-4338.",
                "",
            )
            .replace(
                "The Fayetteville clinic at the Pat Walker Center for Seniors is",
                "",
            )
            .replace("Our clinic on the Lincoln Square is", "")
            .replace("The  clinic is", "")
            .replace("open", "")
            .replace("Sick call is available Monday ", "-")
            .replace(
                " The school-based clinic located on the Lincoln Middle School Campus is  ",
                ",",
            )
            .replace(
                " The  Center for Health clinic is  from ",
                ",",
            )
            .replace(
                "The Women and Infants Center clinic is  ",
                "",
            )
            .replace("Total Spine is  ", "")
            .replace(
                "  other clinic locations vary. For more information, please call 479-463-8740 .",
                "",
            )
            .replace("Sick call is available ", "")
            .replace("The Home is", "")
        )
    else:
        hours_of_operation = "<MISSING>"
    hours_of_operation = (
        hours_of_operation.replace(
            "  other clinic locations vary. For more information, please call 479-463-8740 .",
            "",
        )
        .replace(" Sick call is available", ",")
        .replace(" (or until full).", "")
        .replace("The Home is", "")
        .replace(", or by appointment", "")
    )
    store = [
        DOMAIN,
        page_url,
        location_name,
        street_address,
        city,
        state,
        zip_code,
        country_code,
        store_number,
        phone,
        location_type,
        latitude,
        longitude,
        hours_of_operation,
    ]
    return store


def fetch_type_b(content, page_url, location_type, location_name):
    main = content.find("ul").find_next("p")
    if not main:
        info_addr = content.get_text(strip=True, separator="|").split("|")
        if len(info_addr) >= 19:
            del info_addr[:15]
        info_addr = "|".join(info_addr)
        info_addr = info_addr.replace("Map and Directions", "Telephone:|<MISSING>")
    else:
        addr = main.find_next("p").get_text(strip=True, separator="|").split("|")
        info_addr_list = []
        for row in addr:
            if "72762" in row:
                info_addr_list.append(row)
                info_addr_list.append("Telephone:|479.463.2333")
            else:
                info_addr_list.append(row)
        info_addr = "|".join(info_addr_list)
    info_addr = info_addr.encode("ascii", "ignore").decode()
    full_address = info_addr.replace("Telephone (Washington County):", "Telephone:")
    full_address = full_address.replace("Telephone: ", "Telephone:|")
    full_address = full_address.replace("Phone:", "Telephone:").split("|")
    location = fetch_type_a_proccess(
        page_url,
        content,
        full_address,
        location_name,
        location_type,
    )
    return location


def fetch_type_urgent(soup, page_url):
    main = soup.find_all("script", {"type": "application/ld+json"})[1].string
    data = json.loads(main)
    addr = data["address"]
    location_name = data["name"]
    street_address = addr["streetAddress"].rstrip(", ")
    city = addr["addressLocality"]
    state = addr["addressRegion"]
    zip_code = addr["postalCode"]
    phone = data["telephone"]
    hours_of_operation = " ".join(
        list(soup.find("ul", {"class": "m-location-hours__list"}).stripped_strings)
    )
    location_type = "Urgent Care"
    store_number = "<MISSING>"
    latitude = data["geo"]["latitude"]
    longitude = data["geo"]["longitude"]
    store = [
        DOMAIN,
        page_url,
        location_name,
        street_address,
        city,
        state,
        zip_code,
        "US",
        store_number,
        phone,
        location_type,
        latitude,
        longitude,
        hours_of_operation,
    ]
    return store


def scrape():
    log.info("Start {} Scraper".format(DOMAIN))
    data = fetch_data()
    log.info("Found {} locations".format(len(data)))
    write_output(data)
    log.info("Finish processed " + str(len(data)))


scrape()
